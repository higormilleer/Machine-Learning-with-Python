{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Using XGBoost is easy. Maybe too easy, considering it's generally considered the best ML algorithm around right now.\n\nTo install it, just:\n\npip install xgboost\n\nLet's experiment using the Iris data set. This data set includes the width and length of the petals and sepals of many Iris flowers, and the specific species of Iris the flower belongs to. Our challenge is to predict the species of a flower sample just based on the sizes of its petals. We'll revisit this data set later when we talk about principal component analysis too.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.datasets import load_iris\n\niris = load_iris() #cria uma flor(iris)\n\nnumSamples, numFeatures = iris.data.shape #define o numero de amostras e caracteristicas da flor\nprint(numSamples) #imprime o nuero de amostras(linhas)\nprint(numFeatures) #imprime o numero de caracteristicas(colunas)\nprint(list(iris.target_names)) #exibe os nomes das especies ",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150\n",
            "4\n",
            "['setosa', 'versicolor', 'virginica']\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "Let's divide our data into 20% reserved for testing our model, and the remaining 80% to train it with. By withholding our test data, we can make sure we're evaluating its results based on new flowers it hasn't seen before. Typically we refer to our features (in this case, the petal sizes) as X, and the labels (in this case, the species) as y.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split #divide dados em treinos e teste\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)  # Define que 20% dos dados serão usados para teste e uma semente para tornar a divisão reproduzível",
      "metadata": {},
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "Now we'll load up XGBoost, and convert our data into the DMatrix format it expects. One for the training data, and one for the test data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import xgboost as xgb\n\ntrain = xgb.DMatrix(X_train, label=y_train) #define como matriz que consiste nos dados do recurso de treinamento\ntest = xgb.DMatrix(X_test, label=y_test) #pega todos os recursos de dados do rrotulo",
      "metadata": {},
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "Now we'll define our hyperparameters. We're choosing softmax since this is a multiple classification problem, but the other parameters should ideally be tuned through experimentation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#basicamente define profundidade, taxa de aprendizado, objetivo e o numero de classes\nparam = {\n    'max_depth': 4,\n    'eta': 0.3,\n    'objective': 'multi:softmax',\n    'num_class': 3} \nepochs = 10 #define o numero de iteracoes no treinamento",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's go ahead and train our model using these parameters as a first guess.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = xgb.train(param, train, epochs) #treina o modelo utilizando o paremetros anteriormente colocados em modo de treinameno",
      "metadata": {},
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "Now we'll use the trained model to predict classifications for the data we set aside for testing. Each classification number we get back corresponds to a specific species of Iris.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predictions = model.predict(taest) #gera previsoes a partir do modelo treinado",
      "metadata": {},
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "print(predictions)",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
            " 2. 0. 0. 1. 1. 0.]\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "Let's measure the accuracy on the test data...",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) #calcula a precisao das previsoes",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "Holy crow! It's perfect, and that's just with us guessing as to the best hyperparameters!\n\nNormally I'd have you experiment to find better hyperparameters as an activity, but you can't improve on those results. Instead, see what it takes to make the results worse! How few epochs (iterations) can I get away with? How low can I set the max_depth? Basically try to optimize the simplicity and performance of the model, now that you already have perfect accuracy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#basicamente define profundidade, taxa de aprendizado, objetivo e o numero de classes\nparam = {\n    'max_depth': 4,\n    'eta': 0.3,\n    'objective': 'multi:softmax',\n    'num_class': 3} \nepochs = 8 #define o numero de iteracoes no treinamento\n\nmodel = xgb.train(param, train, epochs) #treina o modelo utilizando o paremetros anteriormente colocados em modo de treinameno\n\npredictions = model.predict(taest) #gera previsoes a partir do modelo treinado\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) #calcula a precisao das previsoes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}